{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "problem_5.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IreneLedyaeva/CAP-5610/blob/master/HW2/problem_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "pwfDXmfJ_zK3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Final CNN architecture with the best performance"
      ]
    },
    {
      "metadata": {
        "id": "K9ylUQ5bf2Rp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The first CNN architecure with data augmentation performs best on the validation set with the training accuracy = 0.7352 and the validation accuracy = 0.7601. Even though the fourth model has the best training accuracy, it does not represent a good generalization."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "U8E235O-XzaC"
      },
      "cell_type": "markdown",
      "source": [
        "## CNN architecture with data augmentation"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "d53DW7eYXzaD"
      },
      "cell_type": "markdown",
      "source": [
        "### Loading the CIFAR10 data set"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "qrQTLJAeXzaE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bb8e9e21-5383-4d09-a4ed-b2b8dd62febd"
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import cifar10\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "170500096/170498071 [==============================] - 27s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "IGpfcJEHXzaM"
      },
      "cell_type": "markdown",
      "source": [
        "### Encoding the labels"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "5H9kdKC4XzaN",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "class_names = ['airplan', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "num_classes = len(class_names)\n",
        "\n",
        "train_labels = to_categorical(train_labels, num_classes)\n",
        "test_labels = to_categorical(test_labels, num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "IHvzPlfHXzaO"
      },
      "cell_type": "markdown",
      "source": [
        "### Preprocessing the data"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "RQpElVYXXzaP",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_images = train_images.astype('float32') / 255.0\n",
        "test_images = test_images.astype('float32') / 255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nx40fYO57VQs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Adding data augmentation\n",
        "Test data should not be augmented"
      ]
    },
    {
      "metadata": {
        "id": "PiRs1w0t7ZmG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Training images\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "datagen.fit(train_images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "EOhwT4mwXzaW"
      },
      "cell_type": "markdown",
      "source": [
        "### Building the model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "d8dc1775-b6c2-4452-c544-ef475523121a",
        "id": "A8Ke8XlUXzaW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    \n",
        "    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    \n",
        "    keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    \n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_3 (Conv2D)            (None, 30, 30, 32)        896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 13, 13, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 4, 4, 128)         73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 2, 2, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 98,378\n",
            "Trainable params: 98,378\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "S6wmyqtMJW6I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Compiling the model"
      ]
    },
    {
      "metadata": {
        "id": "gJjJAvwDJf8P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=RMSprop(lr=0.001),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W9CBde1GJpe-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training the model"
      ]
    },
    {
      "metadata": {
        "id": "B--p3OsrJvTt",
        "colab_type": "code",
        "outputId": "875c5161-0e40-4d7d-fd90-ecef976d262a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1530
        }
      },
      "cell_type": "code",
      "source": [
        "history = model.fit_generator(datagen.flow(train_images, train_labels, batch_size=32),\n",
        "                    epochs=30, verbose=1, validation_data=(test_images, test_labels), shuffle=True)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 7s 746us/sample - loss: 1.3329 - acc: 0.5149\n",
            "1563/1563 [==============================] - 124s 79ms/step - loss: 1.6616 - acc: 0.3977 - val_loss: 1.3330 - val_acc: 0.5149\n",
            "Epoch 2/30\n",
            "10000/10000 [==============================] - 7s 735us/sample - loss: 1.1204 - acc: 0.6066\n",
            "1563/1563 [==============================] - 119s 76ms/step - loss: 1.3227 - acc: 0.5312 - val_loss: 1.1206 - val_acc: 0.6066\n",
            "Epoch 3/30\n",
            "10000/10000 [==============================] - 7s 726us/sample - loss: 1.0418 - acc: 0.6383\n",
            "1563/1563 [==============================] - 123s 79ms/step - loss: 1.1862 - acc: 0.5830 - val_loss: 1.0419 - val_acc: 0.6383\n",
            "Epoch 4/30\n",
            "10000/10000 [==============================] - 7s 724us/sample - loss: 1.0627 - acc: 0.6244\n",
            "1563/1563 [==============================] - 126s 80ms/step - loss: 1.1020 - acc: 0.6162 - val_loss: 1.0628 - val_acc: 0.6244\n",
            "Epoch 5/30\n",
            "10000/10000 [==============================] - 7s 739us/sample - loss: 0.9999 - acc: 0.6556\n",
            "1563/1563 [==============================] - 123s 79ms/step - loss: 1.0519 - acc: 0.6333 - val_loss: 0.9996 - val_acc: 0.6556\n",
            "Epoch 6/30\n",
            "10000/10000 [==============================] - 7s 729us/sample - loss: 0.9095 - acc: 0.6860\n",
            "1563/1563 [==============================] - 123s 79ms/step - loss: 0.9999 - acc: 0.6538 - val_loss: 0.9095 - val_acc: 0.6860\n",
            "Epoch 7/30\n",
            "10000/10000 [==============================] - 7s 732us/sample - loss: 0.9078 - acc: 0.6973\n",
            "1563/1563 [==============================] - 125s 80ms/step - loss: 0.9639 - acc: 0.6653 - val_loss: 0.9072 - val_acc: 0.6973\n",
            "Epoch 8/30\n",
            "10000/10000 [==============================] - 7s 724us/sample - loss: 0.8887 - acc: 0.6969\n",
            "1563/1563 [==============================] - 123s 79ms/step - loss: 0.9365 - acc: 0.6739 - val_loss: 0.8887 - val_acc: 0.6969\n",
            "Epoch 9/30\n",
            "10000/10000 [==============================] - 7s 725us/sample - loss: 0.8759 - acc: 0.7103\n",
            "1563/1563 [==============================] - 124s 80ms/step - loss: 0.9110 - acc: 0.6830 - val_loss: 0.8761 - val_acc: 0.7103\n",
            "Epoch 10/30\n",
            "10000/10000 [==============================] - 7s 720us/sample - loss: 0.8824 - acc: 0.6995\n",
            "1563/1563 [==============================] - 123s 79ms/step - loss: 0.8881 - acc: 0.6939 - val_loss: 0.8827 - val_acc: 0.6995\n",
            "Epoch 11/30\n",
            "10000/10000 [==============================] - 7s 724us/sample - loss: 0.8131 - acc: 0.7270\n",
            "1563/1563 [==============================] - 123s 79ms/step - loss: 0.8732 - acc: 0.6982 - val_loss: 0.8137 - val_acc: 0.7270\n",
            "Epoch 12/30\n",
            "10000/10000 [==============================] - 7s 724us/sample - loss: 0.8149 - acc: 0.7295\n",
            "1563/1563 [==============================] - 124s 79ms/step - loss: 0.8584 - acc: 0.7063 - val_loss: 0.8146 - val_acc: 0.7295\n",
            "Epoch 13/30\n",
            "10000/10000 [==============================] - 7s 724us/sample - loss: 0.8311 - acc: 0.7264\n",
            "1563/1563 [==============================] - 123s 79ms/step - loss: 0.8471 - acc: 0.7095 - val_loss: 0.8311 - val_acc: 0.7264\n",
            "Epoch 14/30\n",
            "10000/10000 [==============================] - 7s 720us/sample - loss: 0.7881 - acc: 0.7352\n",
            "1563/1563 [==============================] - 124s 79ms/step - loss: 0.8390 - acc: 0.7131 - val_loss: 0.7878 - val_acc: 0.7352\n",
            "Epoch 15/30\n",
            "10000/10000 [==============================] - 7s 725us/sample - loss: 0.8206 - acc: 0.7336\n",
            "1563/1563 [==============================] - 122s 78ms/step - loss: 0.8271 - acc: 0.7178 - val_loss: 0.8200 - val_acc: 0.7336\n",
            "Epoch 16/30\n",
            "10000/10000 [==============================] - 7s 725us/sample - loss: 0.8706 - acc: 0.7212\n",
            "1563/1563 [==============================] - 122s 78ms/step - loss: 0.8247 - acc: 0.7154 - val_loss: 0.8699 - val_acc: 0.7212\n",
            "Epoch 17/30\n",
            "10000/10000 [==============================] - 7s 730us/sample - loss: 0.8039 - acc: 0.7302\n",
            "1563/1563 [==============================] - 124s 79ms/step - loss: 0.8073 - acc: 0.7229 - val_loss: 0.8040 - val_acc: 0.7302\n",
            "Epoch 18/30\n",
            "10000/10000 [==============================] - 7s 719us/sample - loss: 0.8643 - acc: 0.7266\n",
            "1563/1563 [==============================] - 122s 78ms/step - loss: 0.8060 - acc: 0.7229 - val_loss: 0.8635 - val_acc: 0.7266\n",
            "Epoch 19/30\n",
            "10000/10000 [==============================] - 7s 721us/sample - loss: 0.8028 - acc: 0.7383\n",
            "1563/1563 [==============================] - 122s 78ms/step - loss: 0.7994 - acc: 0.7304 - val_loss: 0.8025 - val_acc: 0.7383\n",
            "Epoch 20/30\n",
            "10000/10000 [==============================] - 7s 731us/sample - loss: 0.7709 - acc: 0.7547\n",
            "1563/1563 [==============================] - 121s 77ms/step - loss: 0.7995 - acc: 0.7261 - val_loss: 0.7702 - val_acc: 0.7547\n",
            "Epoch 21/30\n",
            "10000/10000 [==============================] - 7s 722us/sample - loss: 0.8119 - acc: 0.7411\n",
            "1563/1563 [==============================] - 121s 78ms/step - loss: 0.7903 - acc: 0.7316 - val_loss: 0.8112 - val_acc: 0.7411\n",
            "Epoch 22/30\n",
            "10000/10000 [==============================] - 7s 724us/sample - loss: 0.8023 - acc: 0.7425\n",
            "1563/1563 [==============================] - 123s 78ms/step - loss: 0.7941 - acc: 0.7310 - val_loss: 0.8019 - val_acc: 0.7425\n",
            "Epoch 23/30\n",
            "10000/10000 [==============================] - 7s 727us/sample - loss: 0.8218 - acc: 0.7265\n",
            "1563/1563 [==============================] - 122s 78ms/step - loss: 0.7886 - acc: 0.7345 - val_loss: 0.8216 - val_acc: 0.7265\n",
            "Epoch 24/30\n",
            "10000/10000 [==============================] - 7s 720us/sample - loss: 0.8820 - acc: 0.7226\n",
            "1563/1563 [==============================] - 123s 79ms/step - loss: 0.7920 - acc: 0.7342 - val_loss: 0.8816 - val_acc: 0.7226\n",
            "Epoch 25/30\n",
            "10000/10000 [==============================] - 7s 717us/sample - loss: 0.9218 - acc: 0.7279\n",
            "1563/1563 [==============================] - 121s 78ms/step - loss: 0.7909 - acc: 0.7320 - val_loss: 0.9213 - val_acc: 0.7279\n",
            "Epoch 26/30\n",
            "10000/10000 [==============================] - 7s 729us/sample - loss: 0.8082 - acc: 0.7380\n",
            "1563/1563 [==============================] - 122s 78ms/step - loss: 0.7810 - acc: 0.7379 - val_loss: 0.8081 - val_acc: 0.7380\n",
            "Epoch 27/30\n",
            "10000/10000 [==============================] - 7s 730us/sample - loss: 0.7740 - acc: 0.7494\n",
            "1563/1563 [==============================] - 123s 79ms/step - loss: 0.7822 - acc: 0.7333 - val_loss: 0.7735 - val_acc: 0.7494\n",
            "Epoch 28/30\n",
            "10000/10000 [==============================] - 7s 720us/sample - loss: 0.8760 - acc: 0.7213\n",
            "1563/1563 [==============================] - 122s 78ms/step - loss: 0.7819 - acc: 0.7376 - val_loss: 0.8754 - val_acc: 0.7213\n",
            "Epoch 29/30\n",
            "10000/10000 [==============================] - 8s 768us/sample - loss: 0.8228 - acc: 0.7352\n",
            "1563/1563 [==============================] - 123s 78ms/step - loss: 0.7839 - acc: 0.7382 - val_loss: 0.8228 - val_acc: 0.7352\n",
            "Epoch 30/30\n",
            "10000/10000 [==============================] - 7s 727us/sample - loss: 0.8093 - acc: 0.7482\n",
            "1563/1563 [==============================] - 121s 78ms/step - loss: 0.7819 - acc: 0.7374 - val_loss: 0.8087 - val_acc: 0.7482\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9_JJfpsRpore",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The final training accuracy: 0.7374\n",
        "\n",
        "The final test accuracy: 0.7482"
      ]
    },
    {
      "metadata": {
        "id": "HdZaiaR5pFcu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Saving the model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "cw7c3VLJXzaf",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_fname = 'model_final_augment.h5' \n",
        "model.save(model_fname)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "I1EZgcrdXzag"
      },
      "cell_type": "markdown",
      "source": [
        "### Downloading the model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ee7JgHLaXzag",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "with open(model_fname, 'r') as f:\n",
        "  files.download(model_fname)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "XEEmq30eXzak"
      },
      "cell_type": "markdown",
      "source": [
        "### Checking overfitting"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "6781e3a3-973d-4a81-cab8-1c7b9dc27dc0",
        "id": "gErcxicdXzak",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 752
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "# Plot training and validation accuracy per epoch\n",
        "plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'ro', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "# Plot training and validation loss per epoch\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'ro', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f05c6965cf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAFnCAYAAAC/5tBZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIuIklNRFREScxP8Hzm7m/ndBIa0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFnCAYAAACPasF4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBgDAYv4/YNmK7NtAZIEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "rtVxvAD2ZPX4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## CNN architecture with k-fold validation and data augmentation"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "qJo1EnI-uHGT"
      },
      "cell_type": "markdown",
      "source": [
        "### Loading the CIFAR10 data set"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "lECszTxquHGU",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import cifar10\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kVOZsyMM18jB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### K-fold validation"
      ]
    },
    {
      "metadata": {
        "id": "j2pjCIjQ2DQ-",
        "colab_type": "code",
        "outputId": "886e41bb-802d-4361-bd9e-b408551fd295",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "folds = list(StratifiedKFold(n_splits=5, shuffle=True, random_state=1).split(train_images, train_labels))\n",
        "\n",
        "print('total training images:', train_images.shape[0])\n",
        "print('total test images:', test_images.shape[0])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total training images: 50000\n",
            "total test images: 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "oeVf160pxkMh"
      },
      "cell_type": "markdown",
      "source": [
        "### Encoding the labels"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "aRoigOrkxkMl",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "class_names = ['airplan', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "num_classes = len(class_names)\n",
        "\n",
        "train_labels = to_categorical(train_labels, num_classes)\n",
        "test_labels = to_categorical(test_labels, num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "H4TG6_wzuHGc"
      },
      "cell_type": "markdown",
      "source": [
        "### Preprocessing the data"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "SSjegInxuHGd",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_images = train_images.astype('float32') / 255.0\n",
        "test_images = test_images.astype('float32') / 255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "NWfCzQ_MuHGg"
      },
      "cell_type": "markdown",
      "source": [
        "### Adding data augmentation\n",
        "Validation data should not be augmented"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "_WJfNWo7uHGh",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Training images\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "datagen.fit(train_images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "PVjrrUIzxuiS"
      },
      "cell_type": "markdown",
      "source": [
        "### Building the model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "4e1d7a7e-e42d-470e-b134-95f8bcf4435f",
        "id": "eSaKrk0hxuiU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        }
      },
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    \n",
        "    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    \n",
        "    keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    \n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 30, 30, 32)        896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 13, 13, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 4, 4, 128)         73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 2, 2, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 98,378\n",
            "Trainable params: 98,378\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "T6Ds-eoYxuia"
      },
      "cell_type": "markdown",
      "source": [
        "### Compiling the model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ThLXOv5_xuic",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=RMSprop(lr=0.001),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tOK5_ixvAzcR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training the model"
      ]
    },
    {
      "metadata": {
        "id": "1giH-P1NI9pl",
        "colab_type": "code",
        "outputId": "6f3b647f-1b92-4acd-b8c6-c52d6837e03d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8061
        }
      },
      "cell_type": "code",
      "source": [
        "for j, (train_idx, val_idx) in enumerate(folds):\n",
        "  \n",
        "  print('\\nFold ',j)\n",
        "  X_train_cv = train_images[train_idx]\n",
        "  y_train_cv = train_labels[train_idx]\n",
        "  X_valid_cv = train_images[val_idx]\n",
        "  y_valid_cv = train_labels[val_idx]\n",
        "  \n",
        "  model.fit_generator(datagen.flow(X_train_cv, y_train_cv, batch_size = 32),\n",
        "                    epochs=30, verbose=1, validation_data=(X_valid_cv, y_valid_cv), shuffle=True)\n",
        "  \n",
        "  print(model.evaluate(X_valid_cv, y_valid_cv))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Fold  0\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/30\n",
            "10000/10000 [==============================] - 7s 681us/sample - loss: 1.4458 - acc: 0.4814\n",
            "1250/1250 [==============================] - 96s 76ms/step - loss: 1.6933 - acc: 0.3809 - val_loss: 1.4455 - val_acc: 0.4814\n",
            "Epoch 2/30\n",
            "10000/10000 [==============================] - 7s 679us/sample - loss: 1.2353 - acc: 0.5602\n",
            "1250/1250 [==============================] - 95s 76ms/step - loss: 1.3813 - acc: 0.5067 - val_loss: 1.2354 - val_acc: 0.5602\n",
            "Epoch 3/30\n",
            "10000/10000 [==============================] - 7s 669us/sample - loss: 1.0991 - acc: 0.6216\n",
            "1250/1250 [==============================] - 95s 76ms/step - loss: 1.2260 - acc: 0.5661 - val_loss: 1.0992 - val_acc: 0.6216\n",
            "Epoch 4/30\n",
            "10000/10000 [==============================] - 7s 674us/sample - loss: 1.0642 - acc: 0.6226\n",
            "1250/1250 [==============================] - 96s 77ms/step - loss: 1.1381 - acc: 0.6023 - val_loss: 1.0648 - val_acc: 0.6226\n",
            "Epoch 5/30\n",
            "10000/10000 [==============================] - 7s 670us/sample - loss: 0.9979 - acc: 0.6542\n",
            "1250/1250 [==============================] - 95s 76ms/step - loss: 1.0733 - acc: 0.6257 - val_loss: 0.9983 - val_acc: 0.6542\n",
            "Epoch 6/30\n",
            "10000/10000 [==============================] - 7s 670us/sample - loss: 1.0013 - acc: 0.6599\n",
            "1250/1250 [==============================] - 95s 76ms/step - loss: 1.0201 - acc: 0.6475 - val_loss: 1.0012 - val_acc: 0.6599\n",
            "Epoch 7/30\n",
            "10000/10000 [==============================] - 7s 676us/sample - loss: 1.2382 - acc: 0.5979\n",
            "1250/1250 [==============================] - 97s 77ms/step - loss: 0.9841 - acc: 0.6600 - val_loss: 1.2392 - val_acc: 0.5979\n",
            "Epoch 8/30\n",
            "10000/10000 [==============================] - 7s 669us/sample - loss: 0.8758 - acc: 0.7018\n",
            "1250/1250 [==============================] - 95s 76ms/step - loss: 0.9481 - acc: 0.6715 - val_loss: 0.8763 - val_acc: 0.7018\n",
            "Epoch 9/30\n",
            "10000/10000 [==============================] - 7s 670us/sample - loss: 0.8739 - acc: 0.7041\n",
            "1250/1250 [==============================] - 95s 76ms/step - loss: 0.9234 - acc: 0.6801 - val_loss: 0.8743 - val_acc: 0.7041\n",
            "Epoch 10/30\n",
            "10000/10000 [==============================] - 7s 696us/sample - loss: 0.9164 - acc: 0.6929\n",
            "1250/1250 [==============================] - 95s 76ms/step - loss: 0.9091 - acc: 0.6844 - val_loss: 0.9168 - val_acc: 0.6929\n",
            "Epoch 11/30\n",
            "10000/10000 [==============================] - 7s 670us/sample - loss: 0.9033 - acc: 0.7074\n",
            "1250/1250 [==============================] - 95s 76ms/step - loss: 0.8863 - acc: 0.6914 - val_loss: 0.9040 - val_acc: 0.7074\n",
            "Epoch 12/30\n",
            "10000/10000 [==============================] - 7s 675us/sample - loss: 0.8402 - acc: 0.7205\n",
            "1250/1250 [==============================] - 95s 76ms/step - loss: 0.8750 - acc: 0.6964 - val_loss: 0.8406 - val_acc: 0.7205\n",
            "Epoch 13/30\n",
            "10000/10000 [==============================] - 7s 671us/sample - loss: 0.8531 - acc: 0.7208\n",
            "1250/1250 [==============================] - 95s 76ms/step - loss: 0.8535 - acc: 0.7079 - val_loss: 0.8535 - val_acc: 0.7208\n",
            "Epoch 14/30\n",
            "10000/10000 [==============================] - 7s 673us/sample - loss: 0.8341 - acc: 0.7190\n",
            "1250/1250 [==============================] - 95s 76ms/step - loss: 0.8481 - acc: 0.7105 - val_loss: 0.8345 - val_acc: 0.7190\n",
            "Epoch 15/30\n",
            "10000/10000 [==============================] - 7s 661us/sample - loss: 0.7895 - acc: 0.7355\n",
            "1250/1250 [==============================] - 94s 75ms/step - loss: 0.8338 - acc: 0.7120 - val_loss: 0.7899 - val_acc: 0.7355\n",
            "Epoch 16/30\n",
            "10000/10000 [==============================] - 7s 674us/sample - loss: 0.8567 - acc: 0.7181\n",
            "1250/1250 [==============================] - 95s 76ms/step - loss: 0.8195 - acc: 0.7185 - val_loss: 0.8575 - val_acc: 0.7181\n",
            "Epoch 17/30\n",
            "10000/10000 [==============================] - 7s 675us/sample - loss: 0.8208 - acc: 0.7342\n",
            "1250/1250 [==============================] - 95s 76ms/step - loss: 0.8197 - acc: 0.7182 - val_loss: 0.8212 - val_acc: 0.7342\n",
            "Epoch 18/30\n",
            "10000/10000 [==============================] - 7s 669us/sample - loss: 0.9445 - acc: 0.7054\n",
            "1250/1250 [==============================] - 95s 76ms/step - loss: 0.8101 - acc: 0.7249 - val_loss: 0.9449 - val_acc: 0.7054\n",
            "Epoch 19/30\n",
            "10000/10000 [==============================] - 7s 676us/sample - loss: 0.8720 - acc: 0.7316\n",
            "1250/1250 [==============================] - 95s 76ms/step - loss: 0.7953 - acc: 0.7266 - val_loss: 0.8721 - val_acc: 0.7316\n",
            "Epoch 20/30\n",
            "10000/10000 [==============================] - 7s 674us/sample - loss: 0.8169 - acc: 0.7369\n",
            "1250/1250 [==============================] - 95s 76ms/step - loss: 0.7942 - acc: 0.7246 - val_loss: 0.8169 - val_acc: 0.7369\n",
            "Epoch 21/30\n",
            "10000/10000 [==============================] - 7s 672us/sample - loss: 0.7670 - acc: 0.7494\n",
            "1250/1250 [==============================] - 95s 76ms/step - loss: 0.7927 - acc: 0.7271 - val_loss: 0.7668 - val_acc: 0.7494\n",
            "Epoch 22/30\n",
            "10000/10000 [==============================] - 7s 680us/sample - loss: 0.8607 - acc: 0.7287\n",
            "1250/1250 [==============================] - 96s 76ms/step - loss: 0.7810 - acc: 0.7333 - val_loss: 0.8612 - val_acc: 0.7287\n",
            "Epoch 23/30\n",
            "10000/10000 [==============================] - 7s 673us/sample - loss: 0.7262 - acc: 0.7678\n",
            "1250/1250 [==============================] - 96s 76ms/step - loss: 0.7782 - acc: 0.7329 - val_loss: 0.7262 - val_acc: 0.7678\n",
            "Epoch 24/30\n",
            "10000/10000 [==============================] - 7s 676us/sample - loss: 0.8788 - acc: 0.7171\n",
            "1250/1250 [==============================] - 96s 77ms/step - loss: 0.7754 - acc: 0.7363 - val_loss: 0.8789 - val_acc: 0.7171\n",
            "Epoch 25/30\n",
            "10000/10000 [==============================] - 7s 673us/sample - loss: 0.8321 - acc: 0.7346\n",
            "1250/1250 [==============================] - 95s 76ms/step - loss: 0.7805 - acc: 0.7329 - val_loss: 0.8324 - val_acc: 0.7346\n",
            "Epoch 26/30\n",
            "10000/10000 [==============================] - 7s 670us/sample - loss: 0.7304 - acc: 0.7610\n",
            "1250/1250 [==============================] - 95s 76ms/step - loss: 0.7782 - acc: 0.7363 - val_loss: 0.7302 - val_acc: 0.7610\n",
            "Epoch 27/30\n",
            "10000/10000 [==============================] - 7s 676us/sample - loss: 0.7841 - acc: 0.7488\n",
            "1250/1250 [==============================] - 95s 76ms/step - loss: 0.7658 - acc: 0.7384 - val_loss: 0.7843 - val_acc: 0.7488\n",
            "Epoch 28/30\n",
            "10000/10000 [==============================] - 7s 671us/sample - loss: 0.9122 - acc: 0.7260\n",
            "1250/1250 [==============================] - 95s 76ms/step - loss: 0.7675 - acc: 0.7415 - val_loss: 0.9128 - val_acc: 0.7260\n",
            "Epoch 29/30\n",
            "10000/10000 [==============================] - 7s 679us/sample - loss: 0.7829 - acc: 0.7570\n",
            "1250/1250 [==============================] - 95s 76ms/step - loss: 0.7639 - acc: 0.7398 - val_loss: 0.7834 - val_acc: 0.7570\n",
            "Epoch 30/30\n",
            "10000/10000 [==============================] - 7s 671us/sample - loss: 0.8214 - acc: 0.7394\n",
            "1250/1250 [==============================] - 95s 76ms/step - loss: 0.7642 - acc: 0.7418 - val_loss: 0.8219 - val_acc: 0.7394\n",
            "10000/10000 [==============================] - 6s 641us/sample - loss: 0.8214 - acc: 0.7394\n",
            "[0.821411733531952, 0.7394]\n",
            "\n",
            "Fold  1\n",
            "Epoch 1/30\n",
            "10000/10000 [==============================] - 7s 666us/sample - loss: 0.6169 - acc: 0.7832\n",
            "1250/1250 [==============================] - 93s 75ms/step - loss: 0.7962 - acc: 0.7321 - val_loss: 0.6174 - val_acc: 0.7832\n",
            "Epoch 2/30\n",
            "10000/10000 [==============================] - 7s 672us/sample - loss: 0.7484 - acc: 0.7475\n",
            "1250/1250 [==============================] - 93s 74ms/step - loss: 0.7820 - acc: 0.7355 - val_loss: 0.7485 - val_acc: 0.7475\n",
            "Epoch 3/30\n",
            "10000/10000 [==============================] - 7s 686us/sample - loss: 0.6938 - acc: 0.7654\n",
            "1250/1250 [==============================] - 93s 75ms/step - loss: 0.7847 - acc: 0.7362 - val_loss: 0.6941 - val_acc: 0.7654\n",
            "Epoch 4/30\n",
            "10000/10000 [==============================] - 7s 680us/sample - loss: 0.6621 - acc: 0.7769\n",
            "1250/1250 [==============================] - 93s 74ms/step - loss: 0.7795 - acc: 0.7365 - val_loss: 0.6626 - val_acc: 0.7769\n",
            "Epoch 5/30\n",
            "10000/10000 [==============================] - 7s 669us/sample - loss: 0.7575 - acc: 0.7505\n",
            "1250/1250 [==============================] - 93s 74ms/step - loss: 0.7743 - acc: 0.7408 - val_loss: 0.7581 - val_acc: 0.7505\n",
            "Epoch 6/30\n",
            "10000/10000 [==============================] - 7s 668us/sample - loss: 0.6981 - acc: 0.7727\n",
            "1250/1250 [==============================] - 93s 74ms/step - loss: 0.7748 - acc: 0.7402 - val_loss: 0.6988 - val_acc: 0.7727\n",
            "Epoch 7/30\n",
            "10000/10000 [==============================] - 7s 661us/sample - loss: 0.7308 - acc: 0.7618\n",
            "1250/1250 [==============================] - 93s 75ms/step - loss: 0.7713 - acc: 0.7416 - val_loss: 0.7310 - val_acc: 0.7618\n",
            "Epoch 8/30\n",
            "10000/10000 [==============================] - 7s 666us/sample - loss: 0.7488 - acc: 0.7549\n",
            "1250/1250 [==============================] - 92s 74ms/step - loss: 0.7691 - acc: 0.7423 - val_loss: 0.7493 - val_acc: 0.7549\n",
            "Epoch 9/30\n",
            "10000/10000 [==============================] - 7s 666us/sample - loss: 0.7047 - acc: 0.7633\n",
            "1250/1250 [==============================] - 93s 74ms/step - loss: 0.7772 - acc: 0.7397 - val_loss: 0.7048 - val_acc: 0.7633\n",
            "Epoch 10/30\n",
            "10000/10000 [==============================] - 7s 663us/sample - loss: 0.8339 - acc: 0.7386\n",
            "1250/1250 [==============================] - 93s 74ms/step - loss: 0.7615 - acc: 0.7460 - val_loss: 0.8345 - val_acc: 0.7386\n",
            "Epoch 11/30\n",
            "10000/10000 [==============================] - 7s 682us/sample - loss: 0.7325 - acc: 0.7550\n",
            "1250/1250 [==============================] - 93s 74ms/step - loss: 0.7820 - acc: 0.7396 - val_loss: 0.7333 - val_acc: 0.7550\n",
            "Epoch 12/30\n",
            "10000/10000 [==============================] - 7s 680us/sample - loss: 0.8271 - acc: 0.7474\n",
            "1250/1250 [==============================] - 93s 74ms/step - loss: 0.7777 - acc: 0.7426 - val_loss: 0.8277 - val_acc: 0.7474\n",
            "Epoch 13/30\n",
            "10000/10000 [==============================] - 7s 702us/sample - loss: 0.7571 - acc: 0.7638\n",
            "1250/1250 [==============================] - 93s 75ms/step - loss: 0.7732 - acc: 0.7430 - val_loss: 0.7572 - val_acc: 0.7638\n",
            "Epoch 14/30\n",
            "10000/10000 [==============================] - 7s 675us/sample - loss: 0.8568 - acc: 0.7379\n",
            "1250/1250 [==============================] - 93s 75ms/step - loss: 0.7784 - acc: 0.7412 - val_loss: 0.8570 - val_acc: 0.7379\n",
            "Epoch 15/30\n",
            "10000/10000 [==============================] - 7s 672us/sample - loss: 0.7296 - acc: 0.7617\n",
            "1250/1250 [==============================] - 93s 75ms/step - loss: 0.7801 - acc: 0.7395 - val_loss: 0.7303 - val_acc: 0.7617\n",
            "Epoch 16/30\n",
            "10000/10000 [==============================] - 7s 676us/sample - loss: 0.7416 - acc: 0.7607\n",
            "1250/1250 [==============================] - 93s 74ms/step - loss: 0.7784 - acc: 0.7389 - val_loss: 0.7419 - val_acc: 0.7607\n",
            "Epoch 17/30\n",
            "10000/10000 [==============================] - 7s 670us/sample - loss: 0.7682 - acc: 0.7514\n",
            "1250/1250 [==============================] - 93s 75ms/step - loss: 0.7811 - acc: 0.7391 - val_loss: 0.7688 - val_acc: 0.7514\n",
            "Epoch 18/30\n",
            "10000/10000 [==============================] - 7s 678us/sample - loss: 0.7600 - acc: 0.7505\n",
            "1250/1250 [==============================] - 93s 74ms/step - loss: 0.7781 - acc: 0.7401 - val_loss: 0.7601 - val_acc: 0.7505\n",
            "Epoch 19/30\n",
            "10000/10000 [==============================] - 7s 678us/sample - loss: 0.7484 - acc: 0.7617\n",
            "1250/1250 [==============================] - 93s 74ms/step - loss: 0.7815 - acc: 0.7390 - val_loss: 0.7485 - val_acc: 0.7617\n",
            "Epoch 20/30\n",
            "10000/10000 [==============================] - 7s 677us/sample - loss: 0.9374 - acc: 0.7357\n",
            "1250/1250 [==============================] - 93s 74ms/step - loss: 0.7790 - acc: 0.7396 - val_loss: 0.9377 - val_acc: 0.7357\n",
            "Epoch 21/30\n",
            "10000/10000 [==============================] - 7s 676us/sample - loss: 0.7586 - acc: 0.7481\n",
            "1250/1250 [==============================] - 93s 74ms/step - loss: 0.7878 - acc: 0.7398 - val_loss: 0.7589 - val_acc: 0.7481\n",
            "Epoch 22/30\n",
            "10000/10000 [==============================] - 7s 675us/sample - loss: 0.7481 - acc: 0.7569\n",
            "1250/1250 [==============================] - 93s 74ms/step - loss: 0.7934 - acc: 0.7369 - val_loss: 0.7485 - val_acc: 0.7569\n",
            "Epoch 23/30\n",
            "10000/10000 [==============================] - 7s 703us/sample - loss: 0.8200 - acc: 0.7494\n",
            "1250/1250 [==============================] - 93s 74ms/step - loss: 0.7964 - acc: 0.7369 - val_loss: 0.8198 - val_acc: 0.7494\n",
            "Epoch 24/30\n",
            "10000/10000 [==============================] - 7s 682us/sample - loss: 0.7699 - acc: 0.7482\n",
            "1250/1250 [==============================] - 93s 75ms/step - loss: 0.7999 - acc: 0.7363 - val_loss: 0.7699 - val_acc: 0.7482\n",
            "Epoch 25/30\n",
            "10000/10000 [==============================] - 7s 685us/sample - loss: 0.9798 - acc: 0.7104\n",
            "1250/1250 [==============================] - 93s 75ms/step - loss: 0.8121 - acc: 0.7326 - val_loss: 0.9799 - val_acc: 0.7104\n",
            "Epoch 26/30\n",
            "10000/10000 [==============================] - 7s 682us/sample - loss: 0.9189 - acc: 0.7084\n",
            "1250/1250 [==============================] - 94s 75ms/step - loss: 0.8147 - acc: 0.7308 - val_loss: 0.9186 - val_acc: 0.7084\n",
            "Epoch 27/30\n",
            "10000/10000 [==============================] - 7s 676us/sample - loss: 0.8636 - acc: 0.7426\n",
            "1250/1250 [==============================] - 94s 75ms/step - loss: 0.8210 - acc: 0.7330 - val_loss: 0.8642 - val_acc: 0.7426\n",
            "Epoch 28/30\n",
            "10000/10000 [==============================] - 7s 680us/sample - loss: 0.8064 - acc: 0.7540\n",
            "1250/1250 [==============================] - 93s 74ms/step - loss: 0.8297 - acc: 0.7285 - val_loss: 0.8078 - val_acc: 0.7540\n",
            "Epoch 29/30\n",
            "10000/10000 [==============================] - 7s 690us/sample - loss: 0.8310 - acc: 0.7376\n",
            "1250/1250 [==============================] - 93s 75ms/step - loss: 0.8348 - acc: 0.7272 - val_loss: 0.8312 - val_acc: 0.7376\n",
            "Epoch 30/30\n",
            "10000/10000 [==============================] - 7s 697us/sample - loss: 0.9035 - acc: 0.7425\n",
            "1250/1250 [==============================] - 94s 75ms/step - loss: 0.8397 - acc: 0.7264 - val_loss: 0.9037 - val_acc: 0.7425\n",
            "10000/10000 [==============================] - 6s 648us/sample - loss: 0.9035 - acc: 0.7425\n",
            "[0.90347551279068, 0.7425]\n",
            "\n",
            "Fold  2\n",
            "10000/10000 [==============================] - 7s 718us/sample - loss: 0.8879 - acc: 0.7457\n",
            "1250/1250 [==============================] - 97s 77ms/step - loss: 0.8662 - acc: 0.7165 - val_loss: 0.8879 - val_acc: 0.7457\n",
            "Epoch 2/30\n",
            "10000/10000 [==============================] - 7s 721us/sample - loss: 0.7341 - acc: 0.7714\n",
            "1250/1250 [==============================] - 97s 77ms/step - loss: 0.8723 - acc: 0.7157 - val_loss: 0.7354 - val_acc: 0.7714\n",
            "Epoch 3/30\n",
            "10000/10000 [==============================] - 7s 722us/sample - loss: 0.8640 - acc: 0.7307\n",
            "1250/1250 [==============================] - 97s 78ms/step - loss: 0.8778 - acc: 0.7150 - val_loss: 0.8647 - val_acc: 0.7307\n",
            "Epoch 4/30\n",
            "10000/10000 [==============================] - 7s 730us/sample - loss: 0.9746 - acc: 0.7262\n",
            "1250/1250 [==============================] - 97s 78ms/step - loss: 0.8907 - acc: 0.7117 - val_loss: 0.9772 - val_acc: 0.7262\n",
            "Epoch 5/30\n",
            "10000/10000 [==============================] - 7s 740us/sample - loss: 1.0450 - acc: 0.7077\n",
            "1250/1250 [==============================] - 97s 77ms/step - loss: 0.9040 - acc: 0.7054 - val_loss: 1.0456 - val_acc: 0.7077\n",
            "Epoch 6/30\n",
            "10000/10000 [==============================] - 8s 775us/sample - loss: 0.8628 - acc: 0.7379\n",
            "1250/1250 [==============================] - 97s 78ms/step - loss: 0.9051 - acc: 0.7062 - val_loss: 0.8645 - val_acc: 0.7379\n",
            "Epoch 7/30\n",
            "10000/10000 [==============================] - 7s 715us/sample - loss: 0.8055 - acc: 0.7491\n",
            "1250/1250 [==============================] - 97s 78ms/step - loss: 0.9112 - acc: 0.7059 - val_loss: 0.8062 - val_acc: 0.7491\n",
            "Epoch 8/30\n",
            "10000/10000 [==============================] - 7s 739us/sample - loss: 0.7835 - acc: 0.7471\n",
            "1250/1250 [==============================] - 97s 77ms/step - loss: 0.9202 - acc: 0.7006 - val_loss: 0.7840 - val_acc: 0.7471\n",
            "Epoch 9/30\n",
            "10000/10000 [==============================] - 7s 735us/sample - loss: 0.9799 - acc: 0.7063\n",
            "1250/1250 [==============================] - 97s 78ms/step - loss: 0.9202 - acc: 0.7024 - val_loss: 0.9806 - val_acc: 0.7063\n",
            "Epoch 10/30\n",
            "10000/10000 [==============================] - 7s 717us/sample - loss: 0.8705 - acc: 0.7355\n",
            "1250/1250 [==============================] - 96s 77ms/step - loss: 0.9248 - acc: 0.7002 - val_loss: 0.8710 - val_acc: 0.7355\n",
            "Epoch 11/30\n",
            "10000/10000 [==============================] - 7s 720us/sample - loss: 0.8230 - acc: 0.7473\n",
            "1250/1250 [==============================] - 96s 77ms/step - loss: 0.9322 - acc: 0.6968 - val_loss: 0.8247 - val_acc: 0.7473\n",
            "Epoch 12/30\n",
            "10000/10000 [==============================] - 7s 733us/sample - loss: 1.0412 - acc: 0.6885\n",
            "1250/1250 [==============================] - 96s 77ms/step - loss: 0.9362 - acc: 0.7019 - val_loss: 1.0422 - val_acc: 0.6885\n",
            "Epoch 13/30\n",
            "10000/10000 [==============================] - 7s 736us/sample - loss: 1.0703 - acc: 0.7128\n",
            "1250/1250 [==============================] - 97s 77ms/step - loss: 0.9316 - acc: 0.7024 - val_loss: 1.0710 - val_acc: 0.7128\n",
            "Epoch 14/30\n",
            "10000/10000 [==============================] - 7s 718us/sample - loss: 0.9139 - acc: 0.7394\n",
            "1250/1250 [==============================] - 96s 77ms/step - loss: 0.9301 - acc: 0.6982 - val_loss: 0.9135 - val_acc: 0.7394\n",
            "Epoch 15/30\n",
            "10000/10000 [==============================] - 7s 718us/sample - loss: 1.1503 - acc: 0.6935\n",
            "1250/1250 [==============================] - 96s 77ms/step - loss: 0.9429 - acc: 0.6944 - val_loss: 1.1511 - val_acc: 0.6935\n",
            "Epoch 16/30\n",
            "10000/10000 [==============================] - 7s 718us/sample - loss: 1.0389 - acc: 0.7234\n",
            "1250/1250 [==============================] - 96s 77ms/step - loss: 0.9523 - acc: 0.6930 - val_loss: 1.0401 - val_acc: 0.7234\n",
            "Epoch 17/30\n",
            "10000/10000 [==============================] - 7s 716us/sample - loss: 0.9371 - acc: 0.7197\n",
            "1250/1250 [==============================] - 96s 77ms/step - loss: 0.9507 - acc: 0.6952 - val_loss: 0.9373 - val_acc: 0.7197\n",
            "Epoch 18/30\n",
            "10000/10000 [==============================] - 7s 712us/sample - loss: 1.3066 - acc: 0.6344\n",
            "1250/1250 [==============================] - 96s 77ms/step - loss: 0.9598 - acc: 0.6916 - val_loss: 1.3073 - val_acc: 0.6344\n",
            "Epoch 19/30\n",
            "10000/10000 [==============================] - 7s 724us/sample - loss: 1.0615 - acc: 0.6949\n",
            "1250/1250 [==============================] - 96s 77ms/step - loss: 0.9669 - acc: 0.6898 - val_loss: 1.0625 - val_acc: 0.6949\n",
            "Epoch 20/30\n",
            "10000/10000 [==============================] - 7s 721us/sample - loss: 0.8421 - acc: 0.7298\n",
            "1250/1250 [==============================] - 96s 76ms/step - loss: 0.9747 - acc: 0.6854 - val_loss: 0.8426 - val_acc: 0.7298\n",
            "Epoch 21/30\n",
            "10000/10000 [==============================] - 7s 715us/sample - loss: 0.8243 - acc: 0.7440\n",
            "1250/1250 [==============================] - 96s 77ms/step - loss: 0.9738 - acc: 0.6867 - val_loss: 0.8251 - val_acc: 0.7440\n",
            "Epoch 22/30\n",
            "10000/10000 [==============================] - 7s 718us/sample - loss: 0.9451 - acc: 0.7348\n",
            "1250/1250 [==============================] - 96s 77ms/step - loss: 0.9692 - acc: 0.6855 - val_loss: 0.9453 - val_acc: 0.7348\n",
            "Epoch 23/30\n",
            "10000/10000 [==============================] - 7s 717us/sample - loss: 1.0399 - acc: 0.6909\n",
            "1250/1250 [==============================] - 98s 78ms/step - loss: 0.9780 - acc: 0.6831 - val_loss: 1.0404 - val_acc: 0.6909\n",
            "Epoch 24/30\n",
            "10000/10000 [==============================] - 7s 719us/sample - loss: 1.0603 - acc: 0.7209\n",
            "1250/1250 [==============================] - 96s 77ms/step - loss: 0.9745 - acc: 0.6856 - val_loss: 1.0611 - val_acc: 0.7209\n",
            "Epoch 25/30\n",
            "10000/10000 [==============================] - 7s 710us/sample - loss: 1.1778 - acc: 0.6756\n",
            "1250/1250 [==============================] - 96s 77ms/step - loss: 0.9823 - acc: 0.6809 - val_loss: 1.1792 - val_acc: 0.6756\n",
            "Epoch 26/30\n",
            "10000/10000 [==============================] - 7s 719us/sample - loss: 1.5885 - acc: 0.6247\n",
            "1250/1250 [==============================] - 96s 77ms/step - loss: 1.0010 - acc: 0.6756 - val_loss: 1.5915 - val_acc: 0.6247\n",
            "Epoch 27/30\n",
            "10000/10000 [==============================] - 7s 738us/sample - loss: 1.0179 - acc: 0.7263\n",
            "1250/1250 [==============================] - 97s 77ms/step - loss: 1.0128 - acc: 0.6757 - val_loss: 1.0179 - val_acc: 0.7263\n",
            "Epoch 28/30\n",
            "10000/10000 [==============================] - 7s 735us/sample - loss: 1.0092 - acc: 0.6983\n",
            "1250/1250 [==============================] - 96s 77ms/step - loss: 1.0404 - acc: 0.6686 - val_loss: 1.0109 - val_acc: 0.6983\n",
            "Epoch 29/30\n",
            "10000/10000 [==============================] - 7s 744us/sample - loss: 1.2226 - acc: 0.6840\n",
            "1250/1250 [==============================] - 97s 78ms/step - loss: 1.0436 - acc: 0.6675 - val_loss: 1.2247 - val_acc: 0.6840\n",
            "Epoch 30/30\n",
            "10000/10000 [==============================] - 8s 772us/sample - loss: 0.9247 - acc: 0.7234\n",
            "1250/1250 [==============================] - 98s 78ms/step - loss: 1.0372 - acc: 0.6666 - val_loss: 0.9255 - val_acc: 0.7234\n",
            "10000/10000 [==============================] - 7s 696us/sample - loss: 0.9247 - acc: 0.7234\n",
            "[0.9246711524009704, 0.7234]\n",
            "\n",
            "Fold  3\n",
            "Epoch 1/30\n",
            "10000/10000 [==============================] - 7s 712us/sample - loss: 0.8762 - acc: 0.7289\n",
            "1250/1250 [==============================] - 95s 76ms/step - loss: 1.0816 - acc: 0.6527 - val_loss: 0.8758 - val_acc: 0.7289\n",
            "Epoch 2/30\n",
            "10000/10000 [==============================] - 7s 715us/sample - loss: 1.0889 - acc: 0.6905\n",
            "1250/1250 [==============================] - 96s 77ms/step - loss: 1.0937 - acc: 0.6502 - val_loss: 1.0877 - val_acc: 0.6905\n",
            "Epoch 3/30\n",
            "10000/10000 [==============================] - 7s 675us/sample - loss: 0.9568 - acc: 0.7064\n",
            "1250/1250 [==============================] - 95s 76ms/step - loss: 1.1054 - acc: 0.6453 - val_loss: 0.9563 - val_acc: 0.7064\n",
            "Epoch 4/30\n",
            "10000/10000 [==============================] - 7s 689us/sample - loss: 1.2261 - acc: 0.6224\n",
            "1250/1250 [==============================] - 95s 76ms/step - loss: 1.1020 - acc: 0.6451 - val_loss: 1.2252 - val_acc: 0.6224\n",
            "Epoch 5/30\n",
            "10000/10000 [==============================] - 7s 703us/sample - loss: 1.2326 - acc: 0.6506\n",
            "1250/1250 [==============================] - 95s 76ms/step - loss: 1.1110 - acc: 0.6435 - val_loss: 1.2316 - val_acc: 0.6506\n",
            "Epoch 6/30\n",
            "10000/10000 [==============================] - 7s 690us/sample - loss: 1.2965 - acc: 0.6481\n",
            "1250/1250 [==============================] - 95s 76ms/step - loss: 1.1081 - acc: 0.6457 - val_loss: 1.2959 - val_acc: 0.6481\n",
            "Epoch 7/30\n",
            "10000/10000 [==============================] - 7s 714us/sample - loss: 1.0729 - acc: 0.6904\n",
            "1250/1250 [==============================] - 95s 76ms/step - loss: 1.1099 - acc: 0.6436 - val_loss: 1.0717 - val_acc: 0.6904\n",
            "Epoch 8/30\n",
            "10000/10000 [==============================] - 7s 718us/sample - loss: 0.9999 - acc: 0.6846\n",
            "1250/1250 [==============================] - 96s 76ms/step - loss: 1.1002 - acc: 0.6432 - val_loss: 0.9996 - val_acc: 0.6846\n",
            "Epoch 9/30\n",
            "10000/10000 [==============================] - 7s 708us/sample - loss: 1.2007 - acc: 0.6450\n",
            "1250/1250 [==============================] - 96s 77ms/step - loss: 1.0947 - acc: 0.6474 - val_loss: 1.2005 - val_acc: 0.6450\n",
            "Epoch 10/30\n",
            "10000/10000 [==============================] - 7s 713us/sample - loss: 1.1426 - acc: 0.6788\n",
            "1250/1250 [==============================] - 95s 76ms/step - loss: 1.1057 - acc: 0.6446 - val_loss: 1.1428 - val_acc: 0.6788\n",
            "Epoch 11/30\n",
            "10000/10000 [==============================] - 7s 702us/sample - loss: 1.1239 - acc: 0.6850\n",
            "1250/1250 [==============================] - 95s 76ms/step - loss: 1.1092 - acc: 0.6439 - val_loss: 1.1242 - val_acc: 0.6850\n",
            "Epoch 12/30\n",
            "10000/10000 [==============================] - 7s 682us/sample - loss: 1.1302 - acc: 0.6420\n",
            "1250/1250 [==============================] - 96s 76ms/step - loss: 1.1173 - acc: 0.6358 - val_loss: 1.1297 - val_acc: 0.6420\n",
            "Epoch 13/30\n",
            "10000/10000 [==============================] - 7s 688us/sample - loss: 0.9805 - acc: 0.7124\n",
            "1250/1250 [==============================] - 94s 75ms/step - loss: 1.1090 - acc: 0.6407 - val_loss: 0.9794 - val_acc: 0.7124\n",
            "Epoch 14/30\n",
            "10000/10000 [==============================] - 7s 683us/sample - loss: 1.0080 - acc: 0.7004\n",
            "1250/1250 [==============================] - 94s 75ms/step - loss: 1.1275 - acc: 0.6358 - val_loss: 1.0074 - val_acc: 0.7004\n",
            "Epoch 15/30\n",
            "10000/10000 [==============================] - 7s 685us/sample - loss: 1.0135 - acc: 0.6771\n",
            "1250/1250 [==============================] - 94s 75ms/step - loss: 1.1301 - acc: 0.6359 - val_loss: 1.0127 - val_acc: 0.6771\n",
            "Epoch 16/30\n",
            "10000/10000 [==============================] - 7s 678us/sample - loss: 1.0363 - acc: 0.6831\n",
            "1250/1250 [==============================] - 95s 76ms/step - loss: 1.1376 - acc: 0.6318 - val_loss: 1.0354 - val_acc: 0.6831\n",
            "Epoch 17/30\n",
            "10000/10000 [==============================] - 7s 702us/sample - loss: 1.2976 - acc: 0.6685\n",
            "1250/1250 [==============================] - 95s 76ms/step - loss: 1.1509 - acc: 0.6300 - val_loss: 1.2964 - val_acc: 0.6685\n",
            "Epoch 18/30\n",
            "10000/10000 [==============================] - 7s 709us/sample - loss: 1.5058 - acc: 0.6351\n",
            "1250/1250 [==============================] - 95s 76ms/step - loss: 1.1385 - acc: 0.6292 - val_loss: 1.5058 - val_acc: 0.6351\n",
            "Epoch 19/30\n",
            "10000/10000 [==============================] - 7s 684us/sample - loss: 1.0993 - acc: 0.6540\n",
            "1250/1250 [==============================] - 95s 76ms/step - loss: 1.1492 - acc: 0.6282 - val_loss: 1.0992 - val_acc: 0.6540\n",
            "Epoch 20/30\n",
            "10000/10000 [==============================] - 7s 677us/sample - loss: 1.1591 - acc: 0.6641\n",
            "1250/1250 [==============================] - 94s 75ms/step - loss: 1.1570 - acc: 0.6254 - val_loss: 1.1582 - val_acc: 0.6641\n",
            "Epoch 21/30\n",
            "10000/10000 [==============================] - 7s 680us/sample - loss: 1.0407 - acc: 0.6722\n",
            "1250/1250 [==============================] - 94s 75ms/step - loss: 1.1536 - acc: 0.6276 - val_loss: 1.0394 - val_acc: 0.6722\n",
            "Epoch 22/30\n",
            "10000/10000 [==============================] - 7s 667us/sample - loss: 1.0171 - acc: 0.6992\n",
            "1250/1250 [==============================] - 94s 76ms/step - loss: 1.1758 - acc: 0.6240 - val_loss: 1.0162 - val_acc: 0.6992\n",
            "Epoch 23/30\n",
            "10000/10000 [==============================] - 7s 685us/sample - loss: 1.3122 - acc: 0.6400\n",
            "1250/1250 [==============================] - 95s 76ms/step - loss: 1.1762 - acc: 0.6216 - val_loss: 1.3110 - val_acc: 0.6400\n",
            "Epoch 24/30\n",
            "10000/10000 [==============================] - 7s 685us/sample - loss: 1.1175 - acc: 0.6684\n",
            "1250/1250 [==============================] - 95s 76ms/step - loss: 1.1765 - acc: 0.6178 - val_loss: 1.1162 - val_acc: 0.6684\n",
            "Epoch 25/30\n",
            "10000/10000 [==============================] - 7s 713us/sample - loss: 1.2850 - acc: 0.6466\n",
            "1250/1250 [==============================] - 95s 76ms/step - loss: 1.1808 - acc: 0.6160 - val_loss: 1.2836 - val_acc: 0.6466\n",
            "Epoch 26/30\n",
            "10000/10000 [==============================] - 7s 681us/sample - loss: 1.0523 - acc: 0.6671\n",
            "1250/1250 [==============================] - 95s 76ms/step - loss: 1.1798 - acc: 0.6165 - val_loss: 1.0510 - val_acc: 0.6671\n",
            "Epoch 27/30\n",
            "10000/10000 [==============================] - 7s 713us/sample - loss: 1.0932 - acc: 0.6588\n",
            "1250/1250 [==============================] - 95s 76ms/step - loss: 1.1753 - acc: 0.6169 - val_loss: 1.0923 - val_acc: 0.6588\n",
            "Epoch 28/30\n",
            "10000/10000 [==============================] - 7s 722us/sample - loss: 1.8710 - acc: 0.5864\n",
            "1250/1250 [==============================] - 95s 76ms/step - loss: 1.1956 - acc: 0.6102 - val_loss: 1.8698 - val_acc: 0.5864\n",
            "Epoch 29/30\n",
            "10000/10000 [==============================] - 7s 713us/sample - loss: 1.2035 - acc: 0.6789\n",
            "1250/1250 [==============================] - 95s 76ms/step - loss: 1.1853 - acc: 0.6115 - val_loss: 1.2025 - val_acc: 0.6789\n",
            "Epoch 30/30\n",
            "10000/10000 [==============================] - 7s 688us/sample - loss: 1.0113 - acc: 0.6907\n",
            "1250/1250 [==============================] - 95s 76ms/step - loss: 1.1961 - acc: 0.6076 - val_loss: 1.0103 - val_acc: 0.6907\n",
            "10000/10000 [==============================] - 7s 660us/sample - loss: 1.0113 - acc: 0.6907\n",
            "[1.0112953857421876, 0.6907]\n",
            "\n",
            "Fold  4\n",
            "Epoch 1/30\n",
            "10000/10000 [==============================] - 8s 765us/sample - loss: 1.1232 - acc: 0.6612\n",
            "1250/1250 [==============================] - 100s 80ms/step - loss: 1.2126 - acc: 0.6033 - val_loss: 1.1233 - val_acc: 0.6612\n",
            "Epoch 2/30\n",
            "10000/10000 [==============================] - 8s 813us/sample - loss: 1.1229 - acc: 0.6681\n",
            "1250/1250 [==============================] - 100s 80ms/step - loss: 1.2111 - acc: 0.5994 - val_loss: 1.1229 - val_acc: 0.6681\n",
            "Epoch 3/30\n",
            "10000/10000 [==============================] - 8s 792us/sample - loss: 1.1967 - acc: 0.6382\n",
            "1250/1250 [==============================] - 99s 79ms/step - loss: 1.2101 - acc: 0.6059 - val_loss: 1.1972 - val_acc: 0.6382\n",
            "Epoch 4/30\n",
            "10000/10000 [==============================] - 7s 717us/sample - loss: 0.9575 - acc: 0.6847\n",
            "1250/1250 [==============================] - 98s 78ms/step - loss: 1.1931 - acc: 0.6095 - val_loss: 0.9575 - val_acc: 0.6847\n",
            "Epoch 5/30\n",
            "10000/10000 [==============================] - 7s 725us/sample - loss: 1.3841 - acc: 0.6042\n",
            "1250/1250 [==============================] - 97s 78ms/step - loss: 1.1853 - acc: 0.6107 - val_loss: 1.3846 - val_acc: 0.6042\n",
            "Epoch 6/30\n",
            "10000/10000 [==============================] - 8s 763us/sample - loss: 1.0743 - acc: 0.6803\n",
            "1250/1250 [==============================] - 97s 78ms/step - loss: 1.2036 - acc: 0.6081 - val_loss: 1.0754 - val_acc: 0.6803\n",
            "Epoch 7/30\n",
            "10000/10000 [==============================] - 8s 779us/sample - loss: 0.9740 - acc: 0.6830\n",
            "1250/1250 [==============================] - 99s 79ms/step - loss: 1.2074 - acc: 0.6000 - val_loss: 0.9741 - val_acc: 0.6830\n",
            "Epoch 8/30\n",
            "10000/10000 [==============================] - 7s 741us/sample - loss: 1.1584 - acc: 0.6527\n",
            "1250/1250 [==============================] - 97s 78ms/step - loss: 1.2107 - acc: 0.6021 - val_loss: 1.1593 - val_acc: 0.6527\n",
            "Epoch 9/30\n",
            "10000/10000 [==============================] - 7s 718us/sample - loss: 1.3978 - acc: 0.6361\n",
            "1250/1250 [==============================] - 97s 77ms/step - loss: 1.2056 - acc: 0.6058 - val_loss: 1.3985 - val_acc: 0.6361\n",
            "Epoch 10/30\n",
            "10000/10000 [==============================] - 7s 704us/sample - loss: 1.0222 - acc: 0.6536\n",
            "1250/1250 [==============================] - 99s 79ms/step - loss: 1.2118 - acc: 0.6040 - val_loss: 1.0226 - val_acc: 0.6536\n",
            "Epoch 11/30\n",
            "10000/10000 [==============================] - 7s 699us/sample - loss: 1.2089 - acc: 0.6590\n",
            "1250/1250 [==============================] - 99s 79ms/step - loss: 1.2191 - acc: 0.6040 - val_loss: 1.2086 - val_acc: 0.6590\n",
            "Epoch 12/30\n",
            "10000/10000 [==============================] - 7s 732us/sample - loss: 1.5033 - acc: 0.6431\n",
            "1250/1250 [==============================] - 99s 79ms/step - loss: 1.2074 - acc: 0.6036 - val_loss: 1.5046 - val_acc: 0.6431\n",
            "Epoch 13/30\n",
            "10000/10000 [==============================] - 8s 806us/sample - loss: 1.3713 - acc: 0.6231\n",
            "1250/1250 [==============================] - 99s 79ms/step - loss: 1.2243 - acc: 0.5982 - val_loss: 1.3717 - val_acc: 0.6231\n",
            "Epoch 14/30\n",
            "10000/10000 [==============================] - 7s 720us/sample - loss: 1.0648 - acc: 0.6816\n",
            "1250/1250 [==============================] - 100s 80ms/step - loss: 1.2328 - acc: 0.5990 - val_loss: 1.0649 - val_acc: 0.6816\n",
            "Epoch 15/30\n",
            "10000/10000 [==============================] - 8s 761us/sample - loss: 1.0805 - acc: 0.6514\n",
            "1250/1250 [==============================] - 98s 78ms/step - loss: 1.2280 - acc: 0.6014 - val_loss: 1.0812 - val_acc: 0.6514\n",
            "Epoch 16/30\n",
            "10000/10000 [==============================] - 7s 720us/sample - loss: 1.2074 - acc: 0.6405\n",
            "1250/1250 [==============================] - 99s 79ms/step - loss: 1.2221 - acc: 0.6003 - val_loss: 1.2071 - val_acc: 0.6405\n",
            "Epoch 17/30\n",
            "10000/10000 [==============================] - 8s 759us/sample - loss: 1.1268 - acc: 0.6523\n",
            "1250/1250 [==============================] - 99s 80ms/step - loss: 1.2266 - acc: 0.5973 - val_loss: 1.1270 - val_acc: 0.6523\n",
            "Epoch 18/30\n",
            "10000/10000 [==============================] - 7s 718us/sample - loss: 1.0070 - acc: 0.6733\n",
            "1250/1250 [==============================] - 99s 79ms/step - loss: 1.1998 - acc: 0.6066 - val_loss: 1.0070 - val_acc: 0.6733\n",
            "Epoch 19/30\n",
            "10000/10000 [==============================] - 7s 731us/sample - loss: 1.2248 - acc: 0.6323\n",
            "1250/1250 [==============================] - 97s 78ms/step - loss: 1.2031 - acc: 0.6072 - val_loss: 1.2252 - val_acc: 0.6323\n",
            "Epoch 20/30\n",
            "10000/10000 [==============================] - 7s 717us/sample - loss: 0.9908 - acc: 0.6853\n",
            "1250/1250 [==============================] - 98s 78ms/step - loss: 1.1935 - acc: 0.6029 - val_loss: 0.9914 - val_acc: 0.6853\n",
            "Epoch 21/30\n",
            "10000/10000 [==============================] - 7s 715us/sample - loss: 1.2229 - acc: 0.6292\n",
            "1250/1250 [==============================] - 99s 79ms/step - loss: 1.1835 - acc: 0.6049 - val_loss: 1.2229 - val_acc: 0.6292\n",
            "Epoch 22/30\n",
            "10000/10000 [==============================] - 8s 796us/sample - loss: 1.3121 - acc: 0.5919\n",
            "1250/1250 [==============================] - 99s 79ms/step - loss: 1.1838 - acc: 0.6065 - val_loss: 1.3121 - val_acc: 0.5919\n",
            "Epoch 23/30\n",
            "10000/10000 [==============================] - 7s 672us/sample - loss: 1.0969 - acc: 0.6559\n",
            "1250/1250 [==============================] - 98s 78ms/step - loss: 1.1939 - acc: 0.6048 - val_loss: 1.0969 - val_acc: 0.6559\n",
            "Epoch 24/30\n",
            "10000/10000 [==============================] - 7s 718us/sample - loss: 1.0989 - acc: 0.6462\n",
            "1250/1250 [==============================] - 99s 79ms/step - loss: 1.1903 - acc: 0.6087 - val_loss: 1.0996 - val_acc: 0.6462\n",
            "Epoch 25/30\n",
            "10000/10000 [==============================] - 7s 736us/sample - loss: 1.0831 - acc: 0.6742\n",
            "1250/1250 [==============================] - 97s 77ms/step - loss: 1.1928 - acc: 0.6104 - val_loss: 1.0837 - val_acc: 0.6742\n",
            "Epoch 26/30\n",
            "10000/10000 [==============================] - 7s 718us/sample - loss: 1.1687 - acc: 0.6421\n",
            "1250/1250 [==============================] - 98s 78ms/step - loss: 1.1814 - acc: 0.6075 - val_loss: 1.1686 - val_acc: 0.6421\n",
            "Epoch 27/30\n",
            "10000/10000 [==============================] - 7s 704us/sample - loss: 1.3104 - acc: 0.5976\n",
            "1250/1250 [==============================] - 99s 80ms/step - loss: 1.1847 - acc: 0.6069 - val_loss: 1.3113 - val_acc: 0.5976\n",
            "Epoch 28/30\n",
            "10000/10000 [==============================] - 7s 701us/sample - loss: 1.0311 - acc: 0.6743\n",
            "1250/1250 [==============================] - 99s 79ms/step - loss: 1.1780 - acc: 0.6073 - val_loss: 1.0313 - val_acc: 0.6743\n",
            "Epoch 29/30\n",
            "10000/10000 [==============================] - 7s 718us/sample - loss: 1.1802 - acc: 0.6468\n",
            "1250/1250 [==============================] - 98s 79ms/step - loss: 1.1777 - acc: 0.6106 - val_loss: 1.1802 - val_acc: 0.6468\n",
            "Epoch 30/30\n",
            "10000/10000 [==============================] - 7s 701us/sample - loss: 0.9667 - acc: 0.6823\n",
            "1250/1250 [==============================] - 99s 79ms/step - loss: 1.2040 - acc: 0.6062 - val_loss: 0.9668 - val_acc: 0.6823\n",
            "10000/10000 [==============================] - 7s 745us/sample - loss: 0.9667 - acc: 0.6823\n",
            "[0.9666766993522644, 0.6823]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5VLMo480GTTu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Fold 0: 0.7394\n",
        "### Fold 1: 0.7425\n",
        "### Fold 2: 0.7234\n",
        "### Fold 3: 0.6907\n",
        "### Fold 4: 0.6823"
      ]
    },
    {
      "metadata": {
        "id": "CPMwnSsPawLB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Saving the model"
      ]
    },
    {
      "metadata": {
        "id": "Imm1TdespJA-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_fname = 'model_final_augment_kfold.h5' \n",
        "model.save(model_fname)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XOY1qRS2pV4L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Downloading the model"
      ]
    },
    {
      "metadata": {
        "id": "dN56DIj4pZt5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "with open(model_fname, 'r') as f:\n",
        "  files.download(model_fname)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S8m6-TcCy-4B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The best validation accuracy obtained by simple hold-out validation is 0.7601. The best validation accuracy obtained by k-fold validation is 0.7425. Therefore, the hold-out validation accuracy is higher than the k-fold validation."
      ]
    }
  ]
}