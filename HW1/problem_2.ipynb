{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "problem_2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IreneLedyaeva/CAP-5610/blob/master/HW1/problem_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "T1L5i6vi7QKU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load the training and test data using Keras (6th classifier)"
      ]
    },
    {
      "metadata": {
        "id": "hhLBHFkm7bYN",
        "colab_type": "code",
        "outputId": "beabb0ac-59f3-476e-dca1-c1e82228312a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "\n",
        "(train_images_original, train_labels_original), (test_images_original, test_labels_original) = mnist.load_data()\n",
        "\n",
        "train_filter = np.where(train_labels_original == 5)\n",
        "test_filter = np.where(test_labels_original == 5)\n",
        "\n",
        "train_images_original, train_labels_original = train_images_original[train_filter], train_labels_original[train_filter]\n",
        "test_images_original, test_labels_original = test_images_original[test_filter], test_labels_original[test_filter]"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gGW5SZY_9Q-b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Preparing the training and test images"
      ]
    },
    {
      "metadata": {
        "id": "ajUwoWMX9UV4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_images = train_images_original.reshape(train_images_original.shape[0], 28 * 28)\n",
        "train_images = train_images.astype('float32') / 255\n",
        "\n",
        "test_images = test_images_original.reshape(test_images_original.shape[0], 28 * 28)\n",
        "test_images = test_images.astype('float32') / 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5RpAJp9I9rdO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Encoding the training and test labels"
      ]
    },
    {
      "metadata": {
        "id": "u5L_gml_-CGc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "train_labels = to_categorical(train_labels_original)\n",
        "test_labels = to_categorical(test_labels_original)\n",
        "\n",
        "train_labels = train_labels_original.reshape(train_labels_original.shape[0], 1)\n",
        "test_labels = test_labels_original.reshape(test_labels_original.shape[0], 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sAALoqpMGl6X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Classifying  the data"
      ]
    },
    {
      "metadata": {
        "id": "326eoGNsGqgZ",
        "colab_type": "code",
        "outputId": "86d7b020-ba88-4dfa-edd9-ec6d12bbfa3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        }
      },
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "  return 1 / (1 + np.exp(-z)) \n",
        "\n",
        "def predict(weight, bias, X):\n",
        "  \n",
        "  m = X.shape[1]\n",
        "  Y_prediction = np.zeros((1, m))\n",
        "  weight = weight.reshape(X.shape[0], 1)\n",
        "  z = np.dot(weight.T, X) + bias\n",
        "  a = sigmoid(z)\n",
        "  \n",
        "  for i in range(a.shape[1]):\n",
        "    if a[0, i] > 0.5:\n",
        "      Y_prediction[0, i] = 1\n",
        "    else:\n",
        "      Y_prediction[0, i] = 0\n",
        "    \n",
        "  return Y_prediction\n",
        "  \n",
        "def train(X, Y, lr):\n",
        "  \n",
        "  epochs = 10\n",
        "  batch_size = 1\n",
        "  m = X.shape[0]\n",
        "  \n",
        "  # Initializing weights and a bias\n",
        "  weight = np.zeros(shape=(X.shape[1], 1), dtype=np.float32)\n",
        "  bias = 0\n",
        "  \n",
        "  for epoch in range(epochs):\n",
        "    \n",
        "    shuffled_indices = np.random.permutation(m)\n",
        "    X_shuffled = X[shuffled_indices, :]\n",
        "    Y_shuffled = Y[shuffled_indices]\n",
        "    \n",
        "    for i in range(0, m, batch_size):\n",
        "      \n",
        "      Xi = X_shuffled[i:i+batch_size, :].T\n",
        "      Yi = Y_shuffled[i:i+batch_size].T \n",
        "      \n",
        "      z = np.dot(weight.T, Xi) + bias\n",
        "      a = sigmoid(z)\n",
        "      loss = (-1) * (np.dot(Yi, np.log(a))) - np.dot(1 - Yi, np.log(1 - a))         \n",
        "      dw = np.dot(Xi, np.transpose(a - Yi))\n",
        "      db = a - Yi\n",
        "      # Updating weights and a bias\n",
        "      weight = 1/batch_size * weight - lr * dw\n",
        "      bias = 1/batch_size * bias - lr * db\n",
        "    \n",
        "  Y_prediction_test = predict(weight, bias, test_images.T)\n",
        "  print(Y_prediction_test)\n",
        "\n",
        "train(train_images, train_labels, lr = 0.01)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:43: RuntimeWarning: divide by zero encountered in log\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            "  1. 1. 1. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}